# SAGE-QG: Structure-Aware Gated Encoding for Knowledge Graph Question Generation(Research Prototype)

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

> **Research Prototype**: ä¸€ä¸ªåŸºäºçŸ¥è¯†å›¾è°±çš„å¤šè·³é—®é¢˜ç”Ÿæˆç³»ç»Ÿï¼Œèåˆäº†å›¾ç¥ç»ç½‘ç»œä¸é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç»“æ„æ„ŸçŸ¥é—¨æ§ç¼–ç æœºåˆ¶ã€‚

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#-é¡¹ç›®ç®€ä»‹)
- [æ ¸å¿ƒç‰¹æ€§](#-æ ¸å¿ƒç‰¹æ€§)
- [ç³»ç»Ÿæ¶æ„](#-ç³»ç»Ÿæ¶æ„)
- [å®éªŒè®¾è®¡](#-å®éªŒè®¾è®¡)
- [é¡¹ç›®ç»“æ„](#-é¡¹ç›®ç»“æ„)
- [ç¯å¢ƒé…ç½®](#-ç¯å¢ƒé…ç½®)
- [æ•°æ®é›†å‡†å¤‡](#-æ•°æ®é›†å‡†å¤‡)
- [æ¨¡å‹è®­ç»ƒ](#-æ¨¡å‹è®­ç»ƒ)
- [è¯„ä¼°æŒ‡æ ‡](#-è¯„ä¼°æŒ‡æ ‡)
- [æ¨¡å‹è¯„ä¼°](#-æ¨¡å‹è¯„ä¼°)
- [å¼•ç”¨](#å¼•ç”¨)

## ğŸ¯ é¡¹ç›®ç®€ä»‹

SAGE-QG æ˜¯ä¸€ä¸ªé’ˆå¯¹çŸ¥è¯†å›¾è°±çš„é—®é¢˜ç”Ÿæˆç³»ç»Ÿï¼Œæ—¨åœ¨ä»ç»™å®šçš„å­å›¾ç»“æ„ä¸­ç”Ÿæˆè‡ªç„¶ã€è¿è´¯ä¸”é€»è¾‘æ­£ç¡®çš„å¤šè·³é—®é¢˜ã€‚æœ¬é¡¹ç›®å®ç°äº†å››ç§ä¸åŒçš„æ¨¡å‹æ¶æ„ï¼Œé€šè¿‡å¯¹æ¯”å®éªŒéªŒè¯äº†ç»“æ„ä¿¡æ¯åœ¨å¤æ‚é—®é¢˜ç”Ÿæˆä»»åŠ¡ä¸­çš„é‡è¦æ€§ã€‚

### ä¸»è¦åˆ›æ–°ç‚¹

1. **ç»“æ„æ„ŸçŸ¥ç¼–ç **ï¼šå°†è·³æ•°è·ç¦»æ‹“æ‰‘ä¿¡æ¯æ³¨å…¥åˆ°æ¨¡å‹è¡¨ç¤ºä¸­
2. **è‡ªé€‚åº”é—¨æ§èåˆ**ï¼šåŠ¨æ€å¹³è¡¡è¯­ä¹‰ç‰¹å¾å’Œå›¾ç»“æ„ç‰¹å¾çš„è´¡çŒ®åº¦
3. **LoRAé«˜æ•ˆå¾®è°ƒ**ï¼šä½¿ç”¨å‚æ•°é«˜æ•ˆçš„LoRAé€‚é…å™¨å¾®è°ƒBARTæ¨¡å‹
4. **LLMè¾…åŠ©è¯„ä¼°**ï¼šå¼•å…¥å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºè£åˆ¤è¿›è¡Œè´¨é‡è¯„åˆ†

## ğŸš€ æ ¸å¿ƒç‰¹æ€§

- **å¤šæ•°æ®é›†æ”¯æŒ**ï¼šæ”¯æŒ PQ, PQL, WC2014, FB15k-237, YAGO3-10, WN18RR ç­‰å¤šä¸ªçŸ¥è¯†å›¾è°±
- **çµæ´»çš„è·³æ•°é…ç½®**ï¼šæ”¯æŒ 2-hop å’Œ 3-hop æ¨ç†è·¯å¾„ç”Ÿæˆ
- **å™ªå£°æ³¨å…¥æœºåˆ¶**ï¼šè‡ªåŠ¨æ·»åŠ å¹²æ‰°èŠ‚ç‚¹å¢å¼ºæ¨¡å‹é²æ£’æ€§
- **å®Œæ•´çš„å®éªŒè¿½è¸ª**ï¼šé›†æˆ WandB è¿›è¡Œè®­ç»ƒç›‘æ§å’Œå¯è§†åŒ–
- **å¤šç»´åº¦è¯„ä¼°**ï¼šBLEU, ROUGE, METEOR, BERTScore, Distinct-N, LLM-Judge

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    A["Knowledge Graph"] --> B["Subgraph Extraction"]
    B --> C["Node Encoding (BART)"]
    C --> D["Structure Injection"]
    D --> E["Graph Reasoning (GAT)"]
    C --> F["Semantic Stream"]
    E --> G["Gated Fusion"]
    F --> G
    G --> H["BART Decoder"]
    H --> I["Question Generation"]
```

### æ¨¡å‹æ¶æ„å¯¹æ¯”

| å®éªŒ     | æ¨¡å‹åç§°        | æ ¸å¿ƒç‰¹ç‚¹         | å…³é”®ç»„ä»¶                 |
| -------- | --------------- | ---------------- | ------------------------ |
| **ExpA** | GNN-Enhanced    | çº¯å›¾ç¥ç»ç½‘ç»œå¢å¼º | GAT + Relation Embedding |
| **ExpB** | Pure BART       | ä»…ä½¿ç”¨è¯­è¨€æ¨¡å‹   | BART Baseline            |
| **ExpC** | Gated Fusion    | é—¨æ§èåˆæœºåˆ¶     | Î± * GNN + (1-Î±) * BART   |
| **ExpD** | Structure-Aware | ç»“æ„ä¿¡æ¯æ³¨å…¥     | Hop Distance |

## ğŸ”¬ å®éªŒè®¾è®¡

### Experiment A: GNN-Enhanced Model

**æ ¸å¿ƒæ€æƒ³**ï¼šä½¿ç”¨ Graph Attention Network (GAT) å¯¹èŠ‚ç‚¹è¿›è¡Œå¤šè·³æ¨ç†å¢å¼ºã€‚

**å…³é”®ç»„ä»¶**ï¼š
- Relation Embeddingï¼šå­¦ä¹ å…³ç³»å‘é‡
- å¤šå±‚ GATv2Convï¼šèåˆè¾¹ä¿¡æ¯çš„å›¾æ³¨æ„åŠ›
- Residual Connectionsï¼šé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±

### Experiment B: Pure BART Baseline

**æ ¸å¿ƒæ€æƒ³**ï¼šä»…ä½¿ç”¨ BART Encoder çš„è¯­ä¹‰ç‰¹å¾ï¼Œä¸è¿›è¡Œå›¾æ¨ç†ã€‚

**è®¾è®¡ç›®çš„**ï¼šä½œä¸ºåŸºçº¿æ¨¡å‹ï¼ŒéªŒè¯å›¾ç»“æ„ä¿¡æ¯çš„å¿…è¦æ€§ã€‚

### Experiment C: Gated Fusion Model

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡å¯å­¦ä¹ çš„é—¨æ§æœºåˆ¶åŠ¨æ€èåˆè¯­ä¹‰æµå’Œé€»è¾‘æµã€‚

**èåˆå…¬å¼**ï¼š
```
Î± = Sigmoid(Linear([Text; Graph]))
Fused = Î± * Graph + (1 - Î±) * Text
```

**ç‰¹ç‚¹**ï¼š
- æ¨¡å‹è‡ªé€‚åº”å†³å®š"çœ‹å­—"è¿˜æ˜¯"çœ‹å›¾"
- æä¾›å¯è§£é‡Šæ€§ï¼ˆé€šè¿‡ Î± å€¼åˆ†æï¼‰

### Experiment D: Structure-Aware Modelï¼ˆæ¨èï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨ ExpC åŸºç¡€ä¸Šæ³¨å…¥ç»“æ„ä¿¡æ¯ï¼ˆè·³æ•°è·ç¦»ï¼‰ã€‚

**ç»“æ„ä¿¡æ¯**ï¼š
- **Hop Distance**ï¼šBFS è®¡ç®—çš„è·³æ•°ï¼ˆ0-9ï¼‰

**æ³¨å…¥æ–¹å¼**ï¼š
```python
x_struct = x_text + x_hop
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
qg/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ background_kbs/          # åŸå§‹çŸ¥è¯†å›¾è°±æ–‡ä»¶
â”‚   â”œâ”€â”€ processed/               # ç”Ÿæˆçš„å­å›¾æ•°æ®ï¼ˆ2h, 3hï¼‰
â”‚   â””â”€â”€ mixed/                   # åˆå¹¶åçš„æ•°æ®é›†ï¼ˆ_mixï¼‰
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ train.py                 # è®­ç»ƒä¸»è„šæœ¬
â”‚   â”œâ”€â”€ eval.py                  # è¯„ä¼°ä¸»è„šæœ¬
â”‚   â”œâ”€â”€ exp_a.py                 # å®éªŒ Aï¼šGNN å¢å¼º
â”‚   â”œâ”€â”€ exp_b.py                 # å®éªŒ Bï¼šBART åŸºçº¿
â”‚   â”œâ”€â”€ exp_c.py                 # å®éªŒ Cï¼šé—¨æ§èåˆ
â”‚   â””â”€â”€ exp_d.py                 # å®éªŒ Dï¼šç»“æ„æ„ŸçŸ¥
â”œâ”€â”€ models/
â”‚   â””â”€â”€ bart.py                  # BART + LoRA å°è£…
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ bertscore/               # BERTScore è¯„ä¼°
â”‚   â”œâ”€â”€ meteor/                  # METEOR è¯„ä¼°
â”‚   â””â”€â”€ rouge/                   # ROUGE è¯„ä¼°
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ build_dataset.py         # æ•°æ®é›†ç”Ÿæˆ
â”‚   â”œâ”€â”€ build_vocab.py           # å…³ç³»è¯è¡¨æ„å»º
â”‚   â”œâ”€â”€ mix_dataset.py           # æ•°æ®é›†åˆå¹¶
â”‚   â”œâ”€â”€ graph_dataset.py         # PyG æ•°æ®åŠ è½½å™¨
â”‚   â””â”€â”€ llm.py                   # LLM API è°ƒç”¨
â”œâ”€â”€ results/                     # è®­ç»ƒå’Œè¯„ä¼°ç»“æœ
â””â”€â”€ build_dataset.sh             # æ‰¹é‡æ•°æ®ç”Ÿæˆè„šæœ¬
```

## ğŸ”§ ç¯å¢ƒé…ç½®

### ä¾èµ–è¦æ±‚

```bash
# æ ¸å¿ƒä¾èµ–
torch>=2.0.0
transformers>=4.30.0
torch-geometric>=2.3.0
datasets>=2.14.0
peft>=0.4.0

# è¯„ä¼°å·¥å…·
evaluate
sacrebleu
nltk
bert-score

# å®éªŒè¿½è¸ª
wandb
seaborn
matplotlib
pandas

# æ•°æ®å¤„ç†
networkx
openai  # ç”¨äº LLM è¯„ä¼°
tqdm
```

### å®‰è£…æ­¥éª¤

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/Episode233/SAGE-QG.git
cd qg

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n kgqg python=3.8
conda activate kgqg

# 3. å®‰è£…ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install torch-geometric
pip install transformers datasets peft evaluate wandb

# 4. ä¸‹è½½ BART æ¨¡å‹ï¼ˆå¦‚éœ€ç¦»çº¿ä½¿ç”¨ï¼‰
# å°† facebook/bart-base ä¸‹è½½åˆ° models/bart-base/
```

## ğŸ“Š æ•°æ®é›†å‡†å¤‡

### æ­¥éª¤ 1ï¼šå‡†å¤‡åŸå§‹çŸ¥è¯†å›¾è°±

å°†æ‚¨çš„çŸ¥è¯†å›¾è°±æ–‡ä»¶ï¼ˆ`.txt` æ ¼å¼ï¼‰æ”¾å…¥ `datasets/background_kbs/` ç›®å½•ã€‚

**æ–‡ä»¶æ ¼å¼**ï¼šæ¯è¡Œä¸€ä¸ªä¸‰å…ƒç»„ï¼Œä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”
```
å¤´å®ä½“\tå…³ç³»\tå°¾å®ä½“
Obama\tborn_in\tHawaii
Hawaii\tlocated_in\tUSA
```

### æ­¥éª¤ 2ï¼šç”Ÿæˆè®­ç»ƒæ•°æ®

ä½¿ç”¨ `build_dataset.py` è‡ªåŠ¨ç”Ÿæˆå­å›¾æ ·æœ¬å¹¶è°ƒç”¨ LLM ç”Ÿæˆé—®é¢˜ï¼š

```bash
# å•ä¸ªæ•°æ®é›†ç”Ÿæˆç¤ºä¾‹
python utils/build_dataset.py -k PQ --hops 2 --attempts 10 --noise 3 --samples 10000

# æ‰¹é‡ç”Ÿæˆï¼ˆæ¨èï¼‰
bash build_dataset.sh
```

**å‚æ•°è¯´æ˜**ï¼š
- `-k, --kb`ï¼šçŸ¥è¯†å›¾è°±æ–‡ä»¶åï¼ˆä¾‹å¦‚ `PQ.txt`ï¼‰
- `--hops`ï¼šæ¨ç†è·³æ•°ï¼ˆ2 æˆ– 3ï¼‰
- `--attempts`ï¼šæ¯ä¸ªèµ·ç‚¹çš„é‡‡æ ·å°è¯•æ¬¡æ•°
- `--noise`ï¼šæ¯ä¸ªèŠ‚ç‚¹æ·»åŠ çš„å™ªå£°é‚»å±…æ•°é‡
- `--samples`ï¼šç›®æ ‡é‡‡æ ·çš„èµ·ç‚¹èŠ‚ç‚¹æ•°é‡

### æ­¥éª¤ 3ï¼šåˆå¹¶å¤šè·³æ•°æ®é›†

å°†åŒä¸€çŸ¥è¯†å›¾è°±çš„ 2-hop å’Œ 3-hop æ•°æ®åˆå¹¶ï¼š

```bash
python utils/mix_dataset.py
```

è¿™ä¼šè‡ªåŠ¨æ‰«æ `datasets/processed/` ç›®å½•ï¼Œå°† `PQ_2h` å’Œ `PQ_3h` åˆå¹¶ä¸º `PQ_mix`ã€‚

### æ­¥éª¤ 4ï¼šæ„å»ºå…³ç³»è¯è¡¨

ä¸ºæ¯ä¸ªæ··åˆæ•°æ®é›†ç”Ÿæˆå…³ç³»åˆ°IDçš„æ˜ å°„ï¼š

```bash
python utils/build_vocab.py -d PQ_mix
python utils/build_vocab.py -d PQL_mix
python utils/build_vocab.py -d WC2014_mix
```

## ğŸ‹ï¸ æ¨¡å‹è®­ç»ƒ

### åŸºç¡€è®­ç»ƒå‘½ä»¤

```bash
python experiments/train.py \
    -e <å®éªŒåç§°> \
    -d <æ•°æ®é›†åç§°> \
    --batch_size 128 \
    --epochs 100 \
    --patience 5 \
    --lr 5e-4 \
    --gnn_layers 3 \
    --dropout 0.1
```

### å®éªŒå‚æ•°è¯´æ˜

| å‚æ•°                 | è¯´æ˜               | é»˜è®¤å€¼ | æ¨èèŒƒå›´              |
| -------------------- | ------------------ | ------ | --------------------- |
| `-e, --exp_name`     | å®éªŒç±»å‹ (a/b/c/d) | a      | -                     |
| `-d, --dataset`      | æ•°æ®é›†åç§°         | -      | PQ_mix, PQL_mix, etc. |
| `--epochs`           | æœ€å¤§è®­ç»ƒè½®æ•°       | 100    | 50-200                |
| `--patience`         | æ—©åœè€å¿ƒå€¼         | 5      | 3-10                  |
| `--batch_size`       | æ‰¹æ¬¡å¤§å°           | 128    | 32-256                |
| `--lr`               | å­¦ä¹ ç‡             | 5e-4   | 1e-4 to 1e-3          |
| `--gnn_layers`       | GNNå±‚æ•°            | 3      | 2-4                   |
| `--grad_accum_steps` | æ¢¯åº¦ç´¯ç§¯æ­¥æ•°       | 1      | 1-4                   |

### è®­ç»ƒç¤ºä¾‹

```bash
# Experiment A (GNN-Enhanced)
python experiments/train.py -e a -d PQ_mix --batch_size 32 --patience 10

# Experiment B (Pure BART)
python experiments/train.py -e b -d PQ_mix --batch_size 32 --patience 10

# Experiment C (Gated Fusion)
python experiments/train.py -e c -d PQL_mix --batch_size 32 --patience 10

# Experiment D (Structure-Aware) - æ¨èé…ç½®
python experiments/train.py -e d -d WC2014_mix --batch_size 128 --patience 5
```

### è®­ç»ƒç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¼šè‡ªåŠ¨è®°å½•åˆ° WandBï¼š
- **Train Loss**ï¼šè®­ç»ƒæŸå¤±æ›²çº¿
- **Val Loss**ï¼šéªŒè¯æŸå¤±æ›²çº¿
- **Learning Rate**ï¼šå­¦ä¹ ç‡è°ƒåº¦
- **Alpha Statistics**ï¼ˆExpC/ExpDï¼‰ï¼šé—¨æ§ç³»æ•°çš„å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å°å€¼ã€æœ€å¤§å€¼
- **Generation Samples**ï¼šæ¯ä¸ª epoch çš„ç”Ÿæˆæ ·ä¾‹

### è¾“å‡ºç»“æœ

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹æƒé‡å’Œæ—¥å¿—ä¼šä¿å­˜åœ¨ `results/` ç›®å½•ï¼š

```
results/
â””â”€â”€ <exp_name>_<dataset>_<timestamp>/
    â”œâ”€â”€ best_model.pt           # æœ€ä½³æ¨¡å‹æƒé‡
    â”œâ”€â”€ loss_curve.png          # æŸå¤±æ›²çº¿å›¾
    â””â”€â”€ training_log.csv        # è®­ç»ƒæ—¥å¿—
```

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

### è‡ªåŠ¨æŒ‡æ ‡

| æŒ‡æ ‡             | å«ä¹‰                         | è¯„ä¼°ç»´åº¦   |
| ---------------- | ---------------------------- | ---------- |
| **BLEU**         | N-gram åŒ¹é…åº¦                | ç”Ÿæˆç²¾åº¦   |
| **ROUGE-1/2/L**  | å¬å›ç‡ï¼ˆå•è¯/åŒè¯/æœ€é•¿åºåˆ—ï¼‰ | è¦†ç›–åº¦     |
| **METEOR**       | è€ƒè™‘åŒä¹‰è¯çš„åŒ¹é…             | è¯­ä¹‰ç›¸ä¼¼åº¦ |
| **BERTScore**    | åŸºäº BERT çš„è¯­ä¹‰ç›¸ä¼¼åº¦       | æ·±å±‚è¯­ä¹‰   |
| **Distinct-1/2** | ä¸é‡å¤ 1/2-gram æ¯”ä¾‹         | å¤šæ ·æ€§     |

### LLM-Judge è¯„ä¼°

ä½¿ç”¨ GPT-4o-mini ä½œä¸ºè£åˆ¤ï¼Œä»äº”ä¸ªç»´åº¦æ‰“åˆ†ï¼ˆæ€»åˆ† 100ï¼‰ï¼š

1. **Fluency & Grammar**ï¼ˆ0-20 åˆ†ï¼‰ï¼šè¯­æ³•æµç•…æ€§
2. **Faithfulness**ï¼ˆ0-20 åˆ†ï¼‰ï¼šæ˜¯å¦å¿ å®äºå›¾ä¸Šä¸‹æ–‡
3. **Logical Correctness**ï¼ˆ0-20 åˆ†ï¼‰ï¼šé€»è¾‘æ˜¯å¦é€šé¡º
4. **Constraints Compliance**ï¼ˆ0-20 åˆ†ï¼‰ï¼šæ˜¯å¦ç¬¦åˆå¤šè·³çº¦æŸä¸”æœªæ³„éœ²ç­”æ¡ˆ
5. **Semantic Alignment**ï¼ˆ0-20 åˆ†ï¼‰ï¼šä¸å‚è€ƒé—®é¢˜çš„è¯­ä¹‰ä¸€è‡´æ€§

## ğŸ“ˆ æ¨¡å‹è¯„ä¼°

### è¯„ä¼°å‘½ä»¤

```bash
python experiments/eval.py \
    -e <å®éªŒåç§°> \
    -d <æ•°æ®é›†åç§°> \
    -c <æ¨¡å‹æƒé‡è·¯å¾„> \
    --batch_size 256 \
    --num_beams 4 \
    --llm_limit 200
```

### å‚æ•°è¯´æ˜

- `-e, --exp_name`ï¼šå®éªŒç±»å‹ï¼ˆa/b/c/dï¼‰
- `-d, --dataset`ï¼šæµ‹è¯•æ•°æ®é›†åç§°
- `-c, --checkpoint`ï¼šè®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ï¼ˆ`best_model.pt`ï¼‰
- `--num_beams`ï¼šBeam Search å®½åº¦ï¼ˆæ¨è 4-5ï¼‰
- `--llm_limit`ï¼šLLM è¯„ä¼°çš„æ ·æœ¬æ•°é‡ï¼ˆ-1 è¡¨ç¤ºå…¨é‡è¯„ä¼°ï¼‰

### è¯„ä¼°ç¤ºä¾‹

```bash
python experiments/eval.py \
    -e d \
    -d PQ_mix \
    -c results/d_PQ_mix_20251217_1605/best_model.pt \
    --batch_size 256 \
    --num_beams 4 \
    --llm_limit 200
```

### è¾“å‡ºç»“æœ

è¯„ä¼°å®Œæˆåä¼šç”Ÿæˆï¼š

```
results/<exp_name>_<dataset>_<timestamp>/
â”œâ”€â”€ test_predictions.csv    # ç”Ÿæˆç»“æœ + LLM è¯„åˆ†
â””â”€â”€ test_metrics.txt        # æ±‡æ€»æŒ‡æ ‡
```

**CSV æ–‡ä»¶åŒ…å«**ï¼š
- `Generated`ï¼šæ¨¡å‹ç”Ÿæˆçš„é—®é¢˜
- `Reference`ï¼šæ ‡å‡†ç­”æ¡ˆé—®é¢˜
- `LLM_Score`ï¼šLLM æ‰“åˆ†ï¼ˆ0-100ï¼‰
- `LLM_Reason`ï¼šLLM è¯„åˆ†ç†ç”±

## ğŸ“ Notes

### å…³é”®æŠ€æœ¯ç»†èŠ‚

1. **LoRA å¾®è°ƒç­–ç•¥**ï¼šä»…å¾®è°ƒ BART çš„ `q_proj` å’Œ `v_proj`ï¼Œå†»ç»“å…¶ä»–å‚æ•°ï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨ã€‚

2. **åŒå‘è¾¹å¤„ç†**ï¼šæ‰€æœ‰è¾¹éƒ½ä¼šè¢«å¤åˆ¶ä¸ºæ­£å‘å’Œåå‘ï¼ˆå¸¦ `_inv` åç¼€ï¼‰ï¼Œå¢å¼ºå›¾è¿é€šæ€§ã€‚

3. **åŠ¨æ€ç»“æ„è®¡ç®—**ï¼šè·³æ•°ä¿¡æ¯åœ¨æ•°æ®åŠ è½½æ—¶å®æ—¶è®¡ç®—ï¼ˆBFSï¼‰ï¼Œæ— éœ€é¢„å¤„ç†ã€‚

4. **Early Stopping**ï¼šä½¿ç”¨éªŒè¯é›†æŸå¤±è§¦å‘æ—©åœï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚

### æ¨èé…ç½®

**å°æ•°æ®é›†ï¼ˆPQ, PQL, WC2014ï¼‰**ï¼š
```bash
--batch_size 32
--patience 10
--grad_accum_steps 1
--lr 5e-4
```

**å¤§æ•°æ®é›†ï¼ˆFB15k-237, YAGO3-10, WN18RRï¼‰**ï¼š
```bash
--batch_size 128
--patience 5
--grad_accum_steps 1
--lr 5e-4
```

### å¸¸è§é—®é¢˜

**Q: CUDA Out of Memoryï¼Ÿ**
- å‡å° `batch_size` æˆ–å¢åŠ  `grad_accum_steps`
- åœ¨ `DataLoader` ä¸­å‡å°‘ `num_workers`

**Q: è®­ç»ƒé€Ÿåº¦æ…¢ï¼Ÿ**
- å¯ç”¨ `pin_memory=True`ï¼ˆå·²é»˜è®¤å¼€å¯ï¼‰
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆéœ€è‡ªè¡Œæ·»åŠ  `torch.cuda.amp`ï¼‰

**Q: LLM è¯„ä¼°å¤±è´¥ï¼Ÿ**
- æ£€æŸ¥ `utils/llm.py` ä¸­çš„ API Key é…ç½®
- ç¡®è®¤ OpenRouter é¢åº¦å……è¶³

## ğŸ“ License

æœ¬é¡¹ç›®é‡‡ç”¨ MIT License å¼€æºåè®®ã€‚

## ğŸ¤ è‡´è°¢

- **BART**ï¼šFacebook AI Research çš„é¢„è®­ç»ƒæ¨¡å‹
- **PyTorch Geometric**ï¼šå›¾ç¥ç»ç½‘ç»œæ¡†æ¶
- **HuggingFace**ï¼šTransformers å’Œ Datasets åº“
- **WandB**ï¼šå®éªŒè¿½è¸ªå¹³å°